{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13825465,"sourceType":"datasetVersion","datasetId":8775786}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q bitsandbytes accelerate peft trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:33:04.961415Z","iopub.execute_input":"2025-11-22T14:33:04.961614Z","iopub.status.idle":"2025-11-22T14:35:02.263362Z","shell.execute_reply.started":"2025-11-22T14:33:04.961596Z","shell.execute_reply":"2025-11-22T14:35:02.262438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:35:02.264960Z","iopub.execute_input":"2025-11-22T14:35:02.265233Z","iopub.status.idle":"2025-11-22T14:35:40.622659Z","shell.execute_reply.started":"2025-11-22T14:35:02.265209Z","shell.execute_reply":"2025-11-22T14:35:40.621859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_id = \"Qwen/Qwen3-4B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:35:40.623407Z","iopub.execute_input":"2025-11-22T14:35:40.623679Z","iopub.status.idle":"2025-11-22T14:36:23.358510Z","shell.execute_reply.started":"2025-11-22T14:35:40.623651Z","shell.execute_reply":"2025-11-22T14:36:23.357928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:36:23.359362Z","iopub.execute_input":"2025-11-22T14:36:23.359722Z","iopub.status.idle":"2025-11-22T14:36:24.056257Z","shell.execute_reply.started":"2025-11-22T14:36:23.359695Z","shell.execute_reply":"2025-11-22T14:36:24.055540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 指令模板 ===\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nYou are a local resident of a region around the world. Answer the following common-sense question based on real-world knowledge in that region. Respond in clear, standard English. Be concise and factual. Provide only the essential answer without any explanation, introduction, or punctuation.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nYou are a local resident of a region around the world. Answer the following common-sense question based on real-world knowledge in that region. Respond in clear, standard English. Be concise and factual. Provide only the essential answer without any explanation, introduction, or punctuation.\n\n### Input:\n{}\n\n### Response:\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:36:24.057963Z","iopub.execute_input":"2025-11-22T14:36:24.058488Z","iopub.status.idle":"2025-11-22T14:36:27.232326Z","shell.execute_reply.started":"2025-11-22T14:36:24.058468Z","shell.execute_reply":"2025-11-22T14:36:27.231346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token\n# === 数据格式化函数 ===\ndef formatting_prompts_func(examples):\n    questions = examples[\"question\"]\n    answers = examples[\"answer\"]\n    outputs = []\n    for q, a in zip(questions, answers):\n        text = alpaca_prompt.format(q, a) + EOS_TOKEN\n        outputs.append(text)\n    return {\"text\": outputs}\n\n# === 加载训练数据 ===\nbuild_data = pd.read_csv(\"/kaggle/input/train-data/build_data.csv\",header=0, delimiter=\"\\t\", quoting=3)\ntest_df = pd.read_csv(\"/kaggle/input/train-data/output_with_translation.tsv\", header=0, delimiter=\"\\t\", quoting=3)\ntrain_df, val_df = train_test_split(build_data, test_size=0.2, random_state=42)\n\ntrain_dict = {'answer': train_df[\"answer_en\"], 'question': train_df['question_en']}\nval_dict = {'answer': val_df[\"answer_en\"], 'question': val_df['question_en']}\ntest_dict = {\"question\": test_df['question_en']}\n\ntrain_dataset = Dataset.from_dict(train_dict)\nval_dataset = Dataset.from_dict(val_dict)\ntest_dataset = Dataset.from_dict(test_dict)\n\n# 格式化\ntrain_dataset = train_dataset.map(formatting_prompts_func, batched=True)\nval_dataset = val_dataset.map(formatting_prompts_func, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:36:27.233308Z","iopub.execute_input":"2025-11-22T14:36:27.234039Z","iopub.status.idle":"2025-11-22T14:36:28.590002Z","shell.execute_reply.started":"2025-11-22T14:36:27.234005Z","shell.execute_reply":"2025-11-22T14:36:28.589214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_dataset[0][\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:36:28.590757Z","iopub.execute_input":"2025-11-22T14:36:28.591038Z","iopub.status.idle":"2025-11-22T14:36:29.532906Z","shell.execute_reply.started":"2025-11-22T14:36:28.591009Z","shell.execute_reply":"2025-11-22T14:36:29.531924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs('/kaggle/working/checkpoint', exist_ok=True)\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/checkpoint\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    num_train_epochs=3,\n    learning_rate=2e-4,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    optim=\"paged_adamw_8bit\",\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    seed=3407,\n    report_to=\"none\",\n)\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:36:29.533990Z","iopub.execute_input":"2025-11-22T14:36:29.534326Z","iopub.status.idle":"2025-11-22T14:36:31.663160Z","shell.execute_reply.started":"2025-11-22T14:36:29.534297Z","shell.execute_reply":"2025-11-22T14:36:31.662417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:36:31.664006Z","iopub.execute_input":"2025-11-22T14:36:31.664275Z","iopub.status.idle":"2025-11-22T14:48:45.992172Z","shell.execute_reply.started":"2025-11-22T14:36:31.664252Z","shell.execute_reply":"2025-11-22T14:48:45.991385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/final_lora\")\ntokenizer.save_pretrained(\"/kaggle/working/final_lora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:48:45.993101Z","iopub.execute_input":"2025-11-22T14:48:45.993839Z","iopub.status.idle":"2025-11-22T14:48:46.632509Z","shell.execute_reply.started":"2025-11-22T14:48:45.993813Z","shell.execute_reply":"2025-11-22T14:48:46.631712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodel.half()\n# === 推理 ===\ntest_df = pd.read_csv(\"/kaggle/input/train-data/output_with_translation.tsv\", sep='\\t')\n\ntest_questions = test_df[\"question_en\"].tolist()\ntest_ids = test_df[\"id\"].tolist()\n\npredictions = []\n\nfor q in tqdm(test_questions, desc=\"Inference\"):\n    prompt = alpaca_prompt.format(q, \"\") \n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=1024,\n        padding=False,\n        add_special_tokens=True,\n    ).to(model.device)\n    \n    input_length = inputs[\"input_ids\"].shape[1]\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            early_stopping=True,\n        )\n\n    generated_tokens = outputs[0][input_length:]\n    answer = tokenizer.decode(\n        generated_tokens,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=False\n    ).strip()\n    \n    predictions.append(answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:48:46.633479Z","iopub.execute_input":"2025-11-22T14:48:46.633771Z","iopub.status.idle":"2025-11-22T14:52:51.675952Z","shell.execute_reply.started":"2025-11-22T14:48:46.633747Z","shell.execute_reply":"2025-11-22T14:52:51.675174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 保存结果 ===\nresult_df = pd.DataFrame({\n    \"id\": test_ids,\n    \"answer_en\": predictions\n})\n\nresult_df.to_csv(\"/kaggle/working/saq_instruction_tuning.csv\", index=False, sep='\\t')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:52:51.676902Z","iopub.execute_input":"2025-11-22T14:52:51.677290Z","iopub.status.idle":"2025-11-22T14:52:51.691796Z","shell.execute_reply.started":"2025-11-22T14:52:51.677261Z","shell.execute_reply":"2025-11-22T14:52:51.691078Z"}},"outputs":[],"execution_count":null}]}